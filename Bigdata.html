<!DOCTYPE html>
<html>
<head>
	<title>大数据</title>
	<meta name="viewport" content="width=device-width,initial-scale=1.0, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no"/>
</head>
<body>
	<h1>第一章</h1>
<h2>大数据的概念</h2>
<p>
	大数据（big data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。
</p>
<h2>大数据的特点</h2>
<p>
	数据量大（Volume）<br>
	数据类型繁多（Variety）<br>
	处理速度快（Velocity）<br>
	价值密度低（Value）<br>
</p>
<h2>
	大数据对科学研究的影响
</h2>
<p>
	第一种范式：实验科学<br>
	第二种范式：理论科学<br>
	第三种范式：计算科学<br>
	第四种范式：数据密集型科学<br>
</p>
<h2>
	大数据对思维方式的影响
</h2>
<p>
	全球而非抽样<br>
	效率而非精确<br>
	相关而非因果<br>
</p>
<h2>大数据对社会发展的影响</h2>
<p>
	大数据决策成为一种新的决策方式<br>
	大数据应用促进信息技术与各行业的深度融合<br>
	大数据开发推动新技术和新应用的不断涌现<br>
</p>
<h2>大数据的应用</h2>
<p>
	制造业----利用工业大数据提升制造业水平，包括产品故障诊断与预测、分析工艺流程、改进生产工艺、优化生产过程能耗、工业供应链分析与优化、生产计划与排程<br>
	金融行业-----大数据在高频交易、社交情绪分析和信贷风险分析三大金融创新领域发挥重要作用<br>
	汽车行业----利用大数据和物联网技术的无人驾驶汽车，再不愿的未来将走入我们的日常生活<br>
	物流行业----利用大数据优化物流王阔，提高物流效率，降低物流成本<br>
	安全领域-----政府可以利用大数据技术构建起强大的国家安全保障体系，企业可以利用大数据地狱网络攻击，警察可以借助大数据来预防犯罪<br>
	个人生活-----大数据还可以应用于个人生活，利用与每个人相关联的“个人大数据”，分析个人生活行为习惯，为其提供更加周到的个性化服务<br>
</p>
<h2>
	大数据关键技术
</h2>
<p>
	数据采集与预处理----利用ETL工具将分布的、异构数据源中的数据，如关系数据、平面数据文件等，抽取到临时中间层后进行清洗、转换、集成，最后加载到数据仓库或数据集市中，成为联机分析处理、数据挖掘的基础；也可以利用日志采集工具（如Flume、Kafka等）把实时采集的数据作为流计算系统的输入，进行实时处理分析<br>
	数据存储和管理-----利用分布式文件系统、数据仓库、关系数据库、NoSQL数据库、云数据库等，实现对结构化、半结构化和非结构化海量数据的存储和管理<br>
	数据处理与分析----利用分布式并行编程模型和计算框架，结合机器学习和数据挖掘算法，实现对海量数据的处理和分析；对分析结果进行可视化呈现，帮助人们更好的理解数据、分析数据<br>
	数据安全和隐私保护-----在从大数据中挖掘在的巨大商业价值和学术价值的同时，构建隐私数据保护体系和数据安全体系，有效保护个人隐私和数据安全<br>
</p>
<h2>
		大数据计算模式
</h2>
<p>
	批处理计算-----针对大规模数据的批量处理------MapReduce、Spark等<br>
	流计算-----针对流数据的实时计算-----Storm、S4、Flume、Streams、Puma、DStream、SuperMario、银河流数据处理平台等<br>
	图计算----针对大规模图结构数据的处理------Pregel、GRaphX、Giraph、PowerGraph、Hama、GoldenOrb等<br>
	查询分析计算------大规模数据的存储管理和查询分析-----Dremel、Hive、Cassandra、Impala等<br>
</p>

<h1>第二章</h1>
<h2>Hadoop简介</h2>
<p>
	Hadoop是Apache软件基金会旗下的一个开源分布式计算平台，为用户提供了系统底层细节透明的分布式基础架构。<br>
	Hadoop的核心是分布式文件系统和MapReduce。<br>
</p>
<h2>Hadoop的特性</h2>
<p>
	高可靠性----采用冗余数据存储方式，即使一个副本发生故障，其他副本也可以保证正常对外提供服务<br>
	高效性----作为并行分布式计算平台，Hadoop采用分布式存储和分布式处理两大核心技术，能够高效地处理PB数据<br>
	高可扩展性----Hadoop的设计目标是可以高效稳定的运行在廉价的计算机集群上，可以扩展到数以千计的计算机节点上<br>
	高容错性----采用冗余数据存储方式，自动保存数据的多个副本，并且能够自动将失败的人数进行重新分配<br>
	成本低----Hadoop采用廉价的计算机集群，成本比较低，普通用户也容易用自己的PC搭建Hadoop运行环境<br>
	运行在Linux平台上----Hadoop是基于Java语言开发的，可以较好的运行在Linux平台上<br>
	支持多种编程语言----Hadoop上的应用程序也可以使用其他语言编写，如C++<br>
</p>
<h2>Hadoop的版本</h2>
<p>
	Apache Hadoop版本分为两代，我们将第一代Hadoop称为Hadoop 1.0，第二代Hadoop称为Hadoop 2.0。第一代Hadoop包含三个大版本，分别是0.20.x，0.21.x和0.22.x，其中，0.20.x最后演化成1.0.x，变成了稳定版，而0.21.x和0.22.x则NameNode HA等新的重大特性。第二代Hadoop包含两个版本，分别是0.23.x和2.x，它们完全不同于Hadoop 1.0，是一套全新的架构，均包含HDFS Federation和YARN两个系统，相比于0.23.x，2.x增加了NameNode HA和Wire-compatibility两个重大特性
</p>

<h2>Hadoop的安装与使用</h2>
<p>
	Hadoop基本安装配置主要包括以下五个步骤<br>
	创建Hadoop用户<br>
	安装Java<br>
	设置SSH登陆权限<br>
	单机安装配置<br>
	伪分布式安装配置<br>
</p>

<h2>Hadoop伪分布式安装</h2>
<p>
	Hadoop中的配置文件
	hadoop-env.sh-----BAsh脚本----记录配置Hadoop运行所需的环境变量，以运行Hadoop<br>
	core-site.xml-----Hadoop配置XML----Hadoop core的配置项，如HDFS和MapReduce常用的I/O设置等<br>
	hdfs-site.xml-----Hadoop配置XMl---Hadoop守护进行的配置项，包括NameNode、Secondary NameNode和DataNode等<br>
	mapred-site.xml-----Hadoop配置XML-----MapReduce守护进程的配置项，包括JobTracker和TaskTracker<br>
	masters----纯文本----运行SecondaryNameNode的机器列表（每行一个）<br>
	slaves----纯文本---运行DataNode和TaskTracker的机器列表（每行一个）<br>
	hadoop-metrics.properties---Java属性-----控制metrics和Hadoop上如何发布的属性<br>
</p>
<p>
	输入jsp指令可以查看所有Java进程<br>
	<xmp>
		$jsp
		8675  NodeManager
		8885  Jps
		8072  NameNode
		8412  SecondaryNameNode    
		8223  DataNode
		8559  ResourceManager
	</xmp>
</p>

<h1>第三章</h1>
<h2>HDFS简介</h2>
<p>
	hdfs支持流数据读取和处理超大规模文件，并能够运行在由廉价的普通机器组成的集群上。<br>
</p>
<h3>HDFS目标</h3>
<p>
	兼容廉价的硬件设备<br>
	流数据读写<br>
	大数据集<br>
	简单的文件模型<br>
	强大的跨平台兼容性<br>
	不适合低延迟数据访问<br>
	无法高校存储大量小文件<br>
</p>
<h3>块</h3>
<p>
	默认的一个块大小是64MB<br>
</p>
<h3>HDFS采用抽象的块概念可以带来的好处</h3>
<p>
	支持大规模文件存储<br>
	简化系统设计<br>
	适合数据备份<br>
</p>
<h3>名称节点和数据节点</h3>
<P>
	名称节点（NodeName）负责管理分布式文件系统的命名空间（Namespace），保存了两个核心的数据结构，即FsImage和EditLog。<br>
	数据节点（DataNode）是分布式文件系统HDFS的工作节点，负责数据的存储和读取，会根据客户端或者名称节点的调度来进行数据的存储和检索，并且向名称节点定期发送自己所存储的块的列表。<br>
	每个数据节点中的数据会保存在各自节点的本地Linux文件系统中。<br>

</P>
<h3>HDFS体系结构</h3>
<p>
	HDFS采用了主从（Master/Slave）结构模型，一个HDFS集群包括一个名称节点和若干个数据节点。
</p>
<h3>HDFS的存储原理</h3>
<p>
	HDFS的存储原理包括数据的荣誉存储、数据存取策略、数据错误与恢复。
</p>
<h3>HDFS常用命令</h3>
<p>
	<xmp>
	hadoop fs -ls<path>    显示<path>指定的文件的详细信息。
	hadoop fs -ls -R<path>    ls命令的地柜版本。


	</xmp>
</p>







<h1>练习题</h1>
<h2>一、名词解释</h2>
<p>
	1、 大数据概念<br>
	大数据（big data），指无法在一定时间范围内用常规软件工具进行捕捉、管理和处理的数据集合，是需要新处理模式才能具有更强的决策力、洞察发现力和流程优化能力的海量、高增长率和多样化的信息资产。<br>

	2、 云数据库概念<br>
	云数据库是指被优化或部署到一个虚拟计算环境中的数据库，可以实现按需付费、按需扩展、高可用性以及存储整合等优势。根据数据库类型一般分为关系型数据库和非关系型数据库（NoSQL数据库)
<br>

	3、 图计算<br>
	“图计算”是以“图论”为基础的对现实世界的一种“图”结构的抽象表达，以及在这种数据结构上的计算模式。

<br>

	4、 ACID持性 <br>
	ACID是Atomic（原子性）、Consistency（一致性）、Isolation（隔离性）和Durability（持久性）的英文缩写。<br>
</p>
<h2>二、判断题</h2>
<p>
	1、 大数据时代带来的思维方式的转变有：全样而非抽样、效率而非准确、相关而非因果.   （对）<br>
	2 、Hadoop是 Apache软件基金会下的一个开源分布式计算平台 ， 为用户提供了系统底层细节透明的分布式基础架构  （对）<br>
	3 、Hadoop的核心包括 HDFS和 MapReduce.    (对)<br>
	4、 HadoopMapReduce的计算模式是基于图结构的计算模式  （错)<br>
	5 、Storm是一个开源的图计算框架  （错） <br>
	6、Hadoop中的ZooKeeper组件是一个髙效而可靠的分布式协同工作系统     (对)<br>
	7、HBase是一个高可靠性、高性能、可伸缩、实时读写、分布式的列式数据库。  （对）<br>
	8、HDFS中对文件逬行了分块存储，用户可以设定分块的大小，一般默认大小为64MB   (对)<br>
	9、 Hadoop在安装时的运行模式，可以单机模式、伪分布模式和完全分布模式    (错)<br>
	10、HDFS采用了主从（Master/Slave) 结 构 模 型 .一 个HDFS集群包括一个名称节点（NameNode) 和 若干个数据节点（DataNode)   (对)<br>
	11、针对Hadoopl.0中单一名称节点 ，存在单点失效问题 .Hadoop2.0 设计了HDFSHA, 提供名称节点热备份  （对）<br>
	12. NoSQL数据库有传统关系型数据库所不具有的灵活的水平扩展性，可以支持海量数据存储，所以可以完全替代关系型数据库。    (错)<br>
	13、MapReduce的设计理念就是“计算向数据靠拢"，而不是“数据向计算靠拢"，即移动计算比移动数据更经济。    (对)<br>
	14、MapReduce计算模型采用了Master (JobTracker) -Slave(TaskTracker)的结构。     (对)<br>
	15、 Topology 是Strom中对Spouts和Bolts组成网络的抽象，它可以被提交到Storm集群中执行。     (对)<br>
	16、Pregel是一种基于BSP模型实现的并行图计算处理系统。    (对)<br> 
	17.数据的可视化是指将大型数据集中的数据以图形图像形式表示，并利用数据分析和开发工具发现其中 未知信息的处理过程     (对)<br>
	18、HBASE中张表的数据可能存放在多台计算机中的。      (错 )<br>
	19、在Map Reduce计算框架中，在Map阶段和Reduce阶段之间有个Shuffle过程。      （对）<br>
	20. Storm和Hadoop类似，也是采用了“Master-Worker”的节点运行方式。  （错）<br>
	</p>
<h2>三、单选题</h2>
<p>
	1、下列哪一项不属于Hadoop分布式处理软件框架的特征     (D).<br>
	A)高可靠性。采用冗余数据存储方式，即使一个副本发生故障，其他副本也可以保证正常对外服务。<br>
	B)高效性，采用分布式并行计算平台，能够高效处理PB级数据<br>
	C)高扩展性可以很容易将集群扩展到数以干计的计算节点上<br>
	D)完全适用于各种分布式计算应用场景。<br>

	2、下列哪个Hadoop子项目是实现分布式文件系统的     (B)<br>
	A)MapReduce<br>
	B) HDFS<br>
	C) HBase<br>  
	D) Zookeeper <br>
	3、下列哪个NoSQL数据库属于键值类型的数据库     (A)。<br>
	A) Redis数据库<br>  
	B) MongoDB<br>
	C) HBase  <br>
	D) Neo4J<br>
	4、在Hadoop配置中，所有节点的HDFS路径是是通过fs .default.name项来配置的，请问它在哪个配置文件中设置的   (A )?<br>
	A) core-site .xml<br>
	B) hdfs-site.xml<br>
	C) mapred-site.xml  <br>
	D) hadoop-env.sh<br>
	5.在一个Hadoop集群中SecondanyNameNode的作用是什么?     （C）<br>
	A)当主NameNode节点出现故障后，作为其备份服务器恢复分布式文件系统的元数据信息。<br>
	B)处理DataNode节点出错恢复的。<br>
	C)对整个Hadoop集群进行协同服务的。<br>
	D)用于与用户之间的服务和通信，<br>
	6、在HBase系统框架中，Region服务器的作用是什么?    (A)<br>
	A)负责所有表和Region服务的管理工作。<br>
	B)服务维护分配给自己的Region,并响应用户的读写请求。<br>
	C) 提供整个HBase集群内的协同服务的。<br>
	D)进行数据的三级寻址服务的。<br>
	7、在Storm框 架中，Stream Groupings的作用是什么?    (A)<br>
	A)定义每个Stream的源头。<br>
	B)定义Streams的状态转换过程。<br>
	C)用户告知Topology如何在两个组件间(如Spout和Bolt之 间或不通的Bolt之间)执行Tuple的传送。<br>
	D)定义一个Tuple数据流序列的。<br>
	8、在MapReduce工作过程中，关于其中Shuffle过程的作用是什么?    (A)<br>
	A)其作用是对Map阶段输出的结果进行分区、排序、合并等处理并交给Reduce的过程<br>
	B)其作用是对输入Map阶段的数据的预处理。<br>
	C)其作用是对Reduce阶段后结果进行排序作用。<br>
	D)其作用是对集群的资源的调度分配过程。<br>

</p>
<h2>四、多选题</h2>
<p>
	1、 大数据具有典型的特征有      (ABCD).<br>
	A)数据量大(Volume)<br>
	B)数据类型繁多（Variety）<br>
	C)处理速度快(Velocity)<br>
	D）价值密度低（Value）<br>
	2、典型的大数据计算模式有    ( ABC)。<br>
	A)批处理计算<br>
	B)流计算<br>
	C)图计算<br>
	D)二叉树计算<br>
	3、数据可视化工具包括有     (ABCD)。<br>
	A)地图工具(Modest Maps、Leaflet等 )<br>
	B) 信息图表工具(Google ChartAPI、 D3、Visual.ly等)<br>
	C)入门级工具(EXCEL)<br>
	D)时间线工具(Timetoast、 Xtimeline等)<br>
	4.归纳起来，典型的NoSQL数据库通常包括有     (ABCD)。<br>
	A) 键值数据库<br>
	B)列族数据库<br>
	C)文档数据库<br>
	D)图形数据库<br>
	5、Hadoop伪分 布模式启动运行后，具有哪些进程?    (BC)<br>
	A) JobTracker<br>
	B) DataNode<br>
	C) NameNode<br>
	D) TaskTracker<br>
</p>

<h2>五、简述题</h2>
<p>
	1、简述HDFS中名称节点和数据节点的具体功能，<br>
	名称节点最主要功能：名称节点记录了每个文件中各个块所在的数据节点的位置信息，数据节点：是HDFS的工作节点，负责数据的存储和读取。<br>

	2、试述Spark应用程序运行基本流程。<br>
	1.构建Spark Application的运行环境（启动SparkContext），SparkContext向资源管理器（可以是Standalone、Mesos或YARN）注册并申请运行Executor资源；<br>


	2.资源管理器分配Executor资源并启动StandaloneExecutorBackend，Executor运行情况将随着心跳发送到资源管理器上；<br>


	3.SparkContext构建成DAG图，将DAG图分解成Stage，并把Taskset发送给Task Scheduler。Executor向SparkContext申请Task，Task Scheduler将Task发放给Executor运行同时SparkContext将应用程序代码发放给Executor；<br>


	4.Task在Executor上运行，运行完毕释放所有资源。<br>


	3、简述在HBase的三层结构中如何访问数据的。<br>

	4、试述NoSQL数据库的四大类型。<br>
	键值数据库<br>
	 相关产品：Redis、Riak、SimpleDB、Chordless、Scalaris、Memcached<br>

   应用：内容缓存<br>

   优点：扩展性好、灵活性好、大量写操作时性能高<br>

   缺点：无法存储结构化信息、条件查询效率较低<br>

   使用者：百度云（Redis）、GitHub（Riak）、BestBuy（Riak）、Twitter（Ridis和Memcached）<br>
<br>
   列族数据库<br>

   相关产品：BigTable、HBase、Cassandra、HadoopDB、GreenPlum、PNUTS<br>

   应用：分布式数据存储与管理<br>

   优点：查找速度快、可扩展性强、容易进行分布式扩展、复杂性低<br>

   使用者：Ebay（Cassandra）、Instagram（Cassandra）、NASA（Cassandra）、Facebook（HBase）<br>
<br>
   文档数据库<br>

   相关产品：MongoDB、CouchDB、ThruDB、CloudKit、Perservere、Jackrabbit<br>

   应用：存储、索引并管理面向文档的数据或者类似的半结构化数据<br>

   优点：性能好、灵活性高、复杂性低、数据结构灵活<br>

   缺点：缺乏统一的查询语言<br>

   使用者：百度云数据库（MongoDB）、SAP（MongoDB）<br><br>

   图形数据库<br>

   相关产品：Neo4J、OrientDB、InfoGrid、GraphDB<br>

   应用：大量复杂、互连接、低结构化的图结构场合，如社交网络、推荐系统等<br>

   优点：灵活性高、支持复杂的图形算法、可用于构建复杂的关系图谱<br>

   缺点：复杂性高、只能支持一定的数据规模<br>

   使用者：Adobe（Neo4J）、Cisco（Neo4J）、T-Mobile（Neo4J）<br><br>
	5、简述MapReducepShufle的过程和功能。<br>
	Shuffle过程，也称Copy阶段。reduce task从各个map task上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定的阀值，则写到磁盘上，否则直接放到内存中。<br>
</p>

<h2>六、程序题</h2>
<p>
	1、写出实现如下功能的shell命令格式。  <br>
	(1)列出hdfs文件系统根目录下的目录和文件<br>
	hadoop fs -ls  /dir <br>
	hadoop fs -ls -R /dir     
	(2)在Ubuntu下查看启动的进程。<br>
	jps
	(3)将本地磁盘的文件复制到hdfs文件系统。<br>
	hadoop fs -put  -f localfile1  localfile2  /user/hadoop/hadoopdir

	(4)将HDFS文件系统的文件复制到本地文件系统。<br>
	hadoop fs -get  hdfs://host:port/user/hadoop/file localfile

	(5)删除HDFS文件系统中的文件，<br>
	hadoop fs -rm  -r /aaa/bbb/

	(6)在HDFS文件系统中创建一一个文件夹。<br>
	hadoop fs -mkdir /test

	(7)在HDFS文件系统中查看文件内容。<br>
	hadoop fs -cat /test/a.php
<br>
    2、对于两个输入文件，即文件A和文件B，请编与MapReduce程序，对两个文件进行合并， 并剔除其中重复的内容， 得到一个新的输出文件C<br>
 <xmp>  
package cn.edu.zucc.mapreduce;
 
import java.io.IOException;
 
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;
 
public class Merge {
 
    public static class Map extends Mapper<Object, Text, Text, Text> {
        private static Text text = new Text();
 
        @Override
        public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
            text = value;
            context.write(text, new Text(""));
        }
    }
 
    public static class Reduce extends Reducer<Text, Text, Text, Text> {
        @Override
        public void reduce(Text key, Iterable<Text> values, Context context) throws IOException, InterruptedException {
            context.write(key, new Text(""));
        }
    }
 
    public static void main(String[] args) throws Exception {
        Configuration conf = new Configuration();
        conf.set("fs.defaultFS", "hdfs://localhost:9000");
        String[] otherArgs = new String[]{"input", "output"};
        if (otherArgs.length != 2) {
            System.err.println("Usage: Merge and duplicate removal <in> <out>");
            System.exit(2);
        }
        Job job = Job.getInstance(conf, "Merge");
        job.setJarByClass(Merge.class);
        job.setMapperClass(Map.class);
        job.setReducerClass(Reduce.class);
        job.setOutputKeyClass(Text.class);
        job.setOutputValueClass(Text.class);
        FileInputFormat.addInputPath(job, new Path(otherArgs[0]));
        FileOutputFormat.setOutputPath(job, new Path(otherArgs[1]));
        System.exit(job.waitForCompletion(true) ? 0 : 1);
    }
 
}
</xmp>
</p>


</body>
</html>


